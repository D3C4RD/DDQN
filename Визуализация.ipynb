{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e1102-4ba0-4085-b9e9-64dc3ade07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c714e8-ff3f-463b-8db5-36a18bdb21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd3df9-f6c1-4f65-9456-89db4508e8fc",
   "metadata": {},
   "source": [
    "# Инициализация нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155cdaf-18a9-4845-82ae-de07164ec308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
    "\n",
    "        # you may want to play around with this and forward()\n",
    "        self.fc1 = nn.Linear(input_dims[0], 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, n_actions)\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # you may want to play around with this\n",
    "    def forward(self, state):\n",
    "        flat1 = F.relu(self.fc1(state))\n",
    "        flat2 = F.relu(self.fc2(flat1))\n",
    "        actions = self.fc3(flat2)\n",
    "        return actions\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f252f1b-c193-43a6-9c83-0042fe2d172b",
   "metadata": {},
   "source": [
    "# Буфер воспроизведения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d32db-ecf6-4205-9bf8-89aa0d5dd543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                         dtype=np.float32)\n",
    "\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122753d-c0a9-45d6-aeca-041bb41254af",
   "metadata": {},
   "source": [
    "# DDQN агент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b77cf-2e62-44d3-a9aa-c2d19ca3de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "\n",
    "class DQNAgent(BaseAgent):\n",
    "    def __init__(self, gamma=0.9, epsilon=0, lr=0.1, n_actions=41, input_dims=(52,),\n",
    "                 mem_size=1000, batch_size=32, eps_min=0.01, eps_dec=5e-7,\n",
    "                 replace=1000, algo='DDQN', env_name='Scenario1b', chkpt_dir='chkpt', load=False):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_dec = eps_dec\n",
    "        self.replace_target_cnt = replace\n",
    "        self.algo = algo\n",
    "        self.env_name = env_name\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)\n",
    "\n",
    "        self.q_eval = DeepQNetwork(self.lr, self.n_actions,\n",
    "                                        input_dims=self.input_dims,\n",
    "                                        name=self.env_name+'_'+self.algo+'_q_eval',\n",
    "                                        chkpt_dir=self.chkpt_dir)\n",
    "        self.q_next = DeepQNetwork(self.lr, self.n_actions,\n",
    "                                        input_dims=self.input_dims,\n",
    "                                        name=self.env_name+'_'+self.algo+'_q_next',\n",
    "                                        chkpt_dir=self.chkpt_dir)\n",
    "\n",
    "    # if epsilon=0 it will just use the model\n",
    "    def get_action(self, observation, action_space=None):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation], dtype=T.float).to(self.q_eval.device)\n",
    "            actions = self.q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    def sample_memory(self):\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states = T.tensor(state).to(self.q_eval.device)\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        dones = T.tensor(done).to(self.q_eval.device)\n",
    "        actions = T.tensor(action).to(self.q_eval.device)\n",
    "        states_ = T.tensor(new_state).to(self.q_eval.device)\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        if self.replace_target_cnt is not None and \\\n",
    "           self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "                           if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def train(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "        self.replace_target_network()\n",
    "        states, actions, rewards, states_, dones = self.sample_memory()\n",
    "        indices = np.arange(self.batch_size)\n",
    "        q_pred = self.q_eval.forward(states)[indices, actions]\n",
    "        q_next = self.q_next.forward(states_)\n",
    "        q_eval = self.q_eval.forward(states_)\n",
    "        max_actions = T.argmax(q_eval, dim=1)\n",
    "        q_next[dones] = 0.0\n",
    "        q_target = rewards + self.gamma*q_next[indices, max_actions]\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "        self.decrement_epsilon()\n",
    "\n",
    "    def end_episode(self):\n",
    "        pass\n",
    "\n",
    "    def set_initial_values(self, action_space, observation):\n",
    "        pass\n",
    "\n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5fce1d-727c-4af9-8f16-d63ca69ce03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CybORG import CybORG\n",
    "from CybORG.Agents import RedMeanderAgent, B_lineAgent\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "from CybORG.Agents.Wrappers.TrueTableWrapper import true_obs_to_table\n",
    "import inspect\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f68851-be38-4bd8-a7fc-156996ce1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(inspect.getfile(CybORG))\n",
    "PATH = PATH[:-10] + '/Shared/Scenarios/Scenario1b.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecce43-426a-482d-865a-76c1347810e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda():\n",
    "    print(\"CUDA: \" + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d69c3b-13e8-491e-9765-a51361fe9f37",
   "metadata": {},
   "source": [
    "# Получение таблиц состояний среды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d6125-02f6-47a5-807e-d9ad4f6c895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables(eps_len=100,chkpt_dir=\"model_meander\", red_agent=RedMeanderAgent):\n",
    "    lr=0.0001\n",
    "    eps_dec=0.000005\n",
    "    eps_min=0.05\n",
    "    gamma=0.99\n",
    "    batch_size=32\n",
    "    epsilon=0\n",
    "    mem_size=5000\n",
    "    replace=1000\n",
    "    cyborg = CybORG(PATH, 'sim', agents={\n",
    "        'Red': red_agent\n",
    "    })\n",
    "    wrapped_cyborg = ChallengeWrapper(env=cyborg, agent_name=\"Blue\")\n",
    "\n",
    "    model_dir = os.path.join(os.getcwd(), \"Models\", chkpt_dir)\n",
    "    print(model_dir)\n",
    "    # the default epsilon is 0. we also don't need to define most hyperparamters since all we will do is agent.get_action()\n",
    "    agent = DQNAgent(gamma=gamma, epsilon=0, lr=lr,\n",
    "                     input_dims=(wrapped_cyborg.observation_space.shape),\n",
    "                     n_actions=wrapped_cyborg.action_space.n, mem_size=mem_size, eps_min=eps_min,\n",
    "                     batch_size=batch_size, replace=replace, eps_dec=eps_dec,\n",
    "                     chkpt_dir=model_dir, algo='DDQNAgent',\n",
    "                     env_name='Scenario1b')\n",
    "    # gets the checkpoint from model_dir\n",
    "    agent.load_models()\n",
    "\n",
    "    blue_moves = []\n",
    "    blue_move_numbers = []\n",
    "    red_moves = []\n",
    "    green_moves = []\n",
    "    table_file = 'visualisation/logs_to_vis/results.txt'\n",
    "    with open(table_file, 'w+') as table_out:\n",
    "        table_out.write('\\n')\n",
    "    observation= wrapped_cyborg.reset()\n",
    "    #print(observation)\n",
    "    agent_name = 'Blue'\n",
    "    action_space = wrapped_cyborg.get_action_space(agent_name)\n",
    "    specialist_agent_names = {0: 'b_lineAgent', 1: 'meanderAgent'}\n",
    "    count_agent_dist = [0,0]\n",
    "    controller_moves = []\n",
    "    moves = []\n",
    "    successes = []\n",
    "    tables = []\n",
    "    total_reward = 0\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    for j in range(100):\n",
    "        action = agent.get_action(observation, action_space)\n",
    "\n",
    "        # Sample the agent selected by our hierarchy controller\n",
    "        observation, rew, done, info = wrapped_cyborg.step(action)\n",
    "\n",
    "        blue_moves += [info['action'].__str__()]\n",
    "        blue_move_numbers += [action]\n",
    "        red_moves += [wrapped_cyborg.get_last_action('Red').__str__()]\n",
    "\n",
    "        green_moves += [wrapped_cyborg.get_last_action('Green').__str__()]\n",
    "\n",
    "        red_move = wrapped_cyborg.get_last_action('Red').__str__()\n",
    "        blue_move = wrapped_cyborg.get_last_action('Blue').__str__()\n",
    "        green_move = wrapped_cyborg.get_last_action('Green').__str__()\n",
    "        true_state = cyborg.get_agent_state('True')\n",
    "        true_table = true_obs_to_table(true_state, cyborg)\n",
    "        success_observation = wrapped_cyborg.get_attr('environment_controller').observation\n",
    "        blue_success = success_observation['Blue'].action_succeeded\n",
    "        red_success = success_observation['Red'].action_succeeded\n",
    "        green_success = success_observation['Green'].action_succeeded\n",
    "        #controller_moves.append(agent_selected_name)\n",
    "        moves.append((blue_move, green_move, red_move))\n",
    "        successes.append((blue_success, green_success, red_success))\n",
    "        tables.append(true_table)\n",
    "        total_reward += rew\n",
    "        rewards.append(rew)\n",
    "\n",
    "\n",
    "    with open(table_file, 'a+') as table_out:\n",
    "        for move in range(len(moves)):\n",
    "            table_out.write('\\n----------------------------------------------------------------------------\\n')\n",
    "            #table_out.write('Agent Selected: {}\\n'.format(controller_moves[move]))\n",
    "            table_out.write('Blue Action: {}\\n'.format(moves[move][0]))\n",
    "            table_out.write('Reward: {}, Episode reward: {}\\n'.format(rewards[move], total_reward))\n",
    "            table_out.write('Network state:\\n')\n",
    "            #table_out.write('Scanned column likely inaccurate.\\n')\n",
    "            table_out.write(str(tables[move]))\n",
    "            table_out.write('\\n.\\n\\n')\n",
    "\n",
    "    print('Controller distribution: {} b_lineAgent, {}, RedMeanderAgent'.format(count_agent_dist[0], count_agent_dist[1]))\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae41f1-4f69-46f9-91b8-ca4e5f9185b3",
   "metadata": {},
   "source": [
    "# Пример получения таблицы стратегии с желаемой наградой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b5359-a670-487e-989f-d02833cbe16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_tables()\n",
    "while a<-50:\n",
    "    a = get_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bd5c1-e400-4b88-8a2c-893329b01def",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ea807-7593-47f4-b1ab-63898924e5a2",
   "metadata": {},
   "source": [
    "# Визуализация таблиц состояний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b6246-c7d1-498c-a12e-e8a4e97f02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f099b06-af60-4c16-853d-37215dd65409",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./visualisation/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6279b44-36ae-42ef-8361-4b45a1d8a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "img = mpl.image.imread(\"./img/figure1.png\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba153e64-86fa-453a-810e-0468b2a2fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('./img/figure1.png')\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im)\n",
    "\n",
    "known_shade = (1,0,0,0.5) # facecolor\n",
    "unknown_shade = (0,0,1,0.5) # facecolor\n",
    "access_none = 'b' # edgecolor\n",
    "access_user = 'y' # edgecolor\n",
    "access_priv = 'r' # edgecolor\n",
    "e0_loc = (460, 55)\n",
    "e1_loc = (532, 55)\n",
    "e2_loc = (603, 55)\n",
    "ops_loc = (968, 55)\n",
    "op_host0_loc = (908, 315)\n",
    "op_host1_loc = (973, 315)\n",
    "op_host2_loc = (1035, 315)\n",
    "user0_loc = (0, 317)\n",
    "user1_loc = (67, 317)\n",
    "user2_loc = (135, 317)\n",
    "user3_loc = (204, 317)\n",
    "user4_loc = (273, 317)\n",
    "defender_loc = (535, 315)\n",
    "server_shape_w = 65\n",
    "server_shape_h = 85\n",
    "host_shape_w = 62\n",
    "host_shape_h = 62\n",
    "\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle(defender_loc, host_shape_w, host_shape_h, linewidth=3, \n",
    "                         edgecolor=access_none, facecolor=unknown_shade)\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe8552-dfd6-4dcc-a327-6f876a83fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_shade = (1,0,0,0.5) # facecolor\n",
    "unknown_shade = (0,0,1,0.5) # facecolor\n",
    "access_none = 'b' # edgecolor\n",
    "access_user = 'y' # edgecolor\n",
    "access_priv = 'r' # edgecolor\n",
    "# e0_loc = (460, 55)\n",
    "# e1_loc = (532, 55)\n",
    "# e2_loc = (603, 55)\n",
    "# ops_loc = (968, 55)\n",
    "# op_host0_loc = (908, 315)\n",
    "# op_host1_loc = (973, 315)\n",
    "# op_host2_loc = (1035, 315)\n",
    "# user0_loc = (0, 317)\n",
    "# user1_loc = (67, 317)\n",
    "# user2_loc = (135, 317)\n",
    "# user3_loc = (204, 317)\n",
    "# user4_loc = (273, 317)\n",
    "# defender_loc = (535, 315)\n",
    "server_shape_w = 65\n",
    "server_shape_h = 85\n",
    "host_shape_w = 62\n",
    "host_shape_h = 62\n",
    "\n",
    "def get_loc(hostname):\n",
    "    if hostname == \"Enterprise0\":\n",
    "        loc = (460, 55)\n",
    "    elif hostname == \"Enterprise1\":\n",
    "        loc = (532, 55)\n",
    "    elif hostname == \"Enterprise2\":\n",
    "        loc = (603, 55)\n",
    "    elif hostname == \"Op_Server0\":\n",
    "        loc = (968, 55)\n",
    "    elif hostname == \"Op_Host0\":\n",
    "        loc = (908, 315)\n",
    "    elif hostname == \"Op_Host1\":\n",
    "        loc = (973, 315)\n",
    "    elif hostname == \"Op_Host2\":\n",
    "        loc = (1035, 315)\n",
    "    elif hostname == \"User0\":\n",
    "        loc = (0, 317)\n",
    "    elif hostname == \"User1\":\n",
    "        loc = (67, 317)\n",
    "    elif hostname == \"User2\":\n",
    "        loc = (135, 317)\n",
    "    elif hostname == \"User3\":\n",
    "        loc = (204, 317)\n",
    "    elif hostname == \"User4\":\n",
    "        loc = (273, 317)\n",
    "    elif hostname == \"Defender\":\n",
    "        loc = (535, 315) \n",
    "    return loc\n",
    "\n",
    "def add_patch(hostname, known_state, access_state, scanned=False):\n",
    "    # Translate variables to colors\n",
    "    if known_state:\n",
    "        facecolor_shade = known_shade\n",
    "    else:\n",
    "        facecolor_shade = unknown_shade\n",
    "    if access_state == \"None\":\n",
    "        edgecolor_shade = access_none\n",
    "    elif access_state == \"User\":\n",
    "        edgecolor_shade = access_user\n",
    "    else: # access_state == \"Privileged\"\n",
    "        edgecolor_shade = access_priv\n",
    "        \n",
    "    if \"Enterprise\" in hostname or \"Server\" in hostname:\n",
    "        shape_w = server_shape_w\n",
    "        shape_h = server_shape_h\n",
    "    else: # Host\n",
    "        shape_w = host_shape_w\n",
    "        shape_h = host_shape_h\n",
    "    loc = get_loc(hostname)\n",
    "    \n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle(loc, shape_w, shape_h, linewidth=3, \n",
    "                             edgecolor=edgecolor_shade, facecolor=facecolor_shade)\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e09c3e-c875-4e31-8569-962a9dec3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results(file_name, dest_file_name=\"\"):\n",
    "    with open(file_name, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    results_json = []\n",
    "    step = {\"hosts\":[]}\n",
    "    header = False\n",
    "    for i in range(len(lines)):\n",
    "        l = lines[i]\n",
    "        if \"Blue Action: \" in l:\n",
    "            step[\"blue_action\"] = l.lstrip('Blue Action: ').strip()\n",
    "            #print( l.lstrip('Blue Action: ').strip())\n",
    "        elif \"Reward: \" in l:\n",
    "            rewards = l.split(\",\")\n",
    "            step[\"reward\"] = rewards[0].lstrip(\"Reward: \")\n",
    "            step[\"ep_reward\"] = rewards[1].strip().lstrip(\"Episode reward: \")\n",
    "            #print(f\"parsed: {step['reward']} and {step['ep_reward']}\")\n",
    "        elif \"+\" in l:\n",
    "            if \"Subnet\" in lines[i+1]:\n",
    "                header = True\n",
    "            else: # Header started so this is end of header\n",
    "                header = False\n",
    "        elif not header and \"|\" in l:\n",
    "            row = l.split(\"|\")\n",
    "            attr = {} # attributes listed by hostname\n",
    "            attr[\"subnet\"] = row[1].strip()\n",
    "            attr[\"ip_addr\"] = row[2].strip()\n",
    "            attr[\"known\"] = \"True\" in row[4].strip()\n",
    "    #         print(\"True\" in attr[\"known\"])\n",
    "            attr[\"scanned\"] = \"True\" in row[5].strip()\n",
    "            attr[\"access\"] = row[6].strip()\n",
    "            attr[\"hostname\"] = row[3].strip()\n",
    "            step[\"hosts\"].append(attr)\n",
    "    #         print(f\"{hostname} ({ip_addr} with subnet {subnet}) is known ({known}), scanned ({scanned}), and access ({access})\")\n",
    "    #         print(row)\n",
    "            #print(step['hosts'][0])\n",
    "        elif '----------------------------------------------------------------------------' in l and not step == {\"hosts\":[]}:\n",
    "            #print('я работаю')\n",
    "            results_json.append(step)\n",
    "            step = {\"hosts\":[]}\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "\n",
    "    if dest_file_name == \"\":\n",
    "        dest_file_name = file_name.rstrip(\".txt\") + \".json\"\n",
    "    with open(dest_file_name, 'w') as fp:\n",
    "        fp.write(str(results_json))\n",
    "    return results_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec0686-97bf-462e-aaf2-baf14c332c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_json = parse_results(file_name='./logs_to_vis/-30.1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078411c-297a-46ef-9949-824d8e538df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb881cb-5692-427e-a70d-0a37b90102f8",
   "metadata": {},
   "source": [
    "# Получение картинок из таблиц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef99e25-d125-4df9-bebd-9dcc5106deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.0\n",
    "for i in range(len(results_json)):\n",
    "    img = results_json[i]\n",
    "    base = Image.open('./img/figure1.png')\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(20, 8))\n",
    "    # Display the image\n",
    "    ax.imshow(base)\n",
    "    \n",
    "    for host in img[\"hosts\"]:\n",
    "    #     def add_patch(hostname, known_state, access_state, scanned=False):\n",
    "#         print(host)\n",
    "        add_patch(host[\"hostname\"], host[\"known\"], host[\"access\"], host[\"scanned\"])\n",
    "    r+=float(img['reward'])\n",
    "    if i == 0:\n",
    "        plt.title(\"Starting...\")\n",
    "    else:\n",
    "        plt.title(f\"Blue Action: {img['blue_action']} \\nReward: {img['reward']} \\nEp Reward: {r} \\nStep:{i}\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f\"./img/strategy//img{i}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a8d1c-15ec-4731-bc5d-8c579ba1cbce",
   "metadata": {},
   "source": [
    "# сохранить картинки в GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bc785-b767-4469-9679-874fe968a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "# filepaths\n",
    "fp_in = \"img/strategy/img*.png\"\n",
    "fp_out = \"results.gif\"\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "# Функция для сортировки с учетом чисел в названии файлов\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "# Сортируем файлы по числовому порядку\n",
    "file_list = sorted(glob.glob(fp_in), key=natural_keys)\n",
    "\n",
    "img, *imgs = [Image.open(f) for f in file_list]\n",
    "img.save(fp=fp_out, format='GIF', append_images=imgs,\n",
    "         save_all=True, duration=400, loop=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
